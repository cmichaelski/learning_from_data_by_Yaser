{
 "metadata": {
  "name": "",
  "signature": "sha256:5eac2ca92548ea3dd5b460ab3839d20020d106302d3672baeb7aca748fa16b39"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Homework 7"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "from cvxopt import matrix, solvers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Validation\n",
      "In the following problems, use the data provided in the files in.dta and out.dta for Homework # 6 .We are going to apply linear regression with a nonlinear transformation for classification (without regularization). The nonlinear transformation is given by $\\\\phi_0$ through $\\\\phi_7$ which transform $(x_1, x_2)$ into $$1, x_1 , x_2 , x_1^2 , x_2^2 , x_1x_2 , |x_1 - x_2|, |x_1 + x_2|.$$\n",
      "To illustrate how taking out points for validation affects the performance, we will consider the hypotheses trained on $\\mathcal D_{train}$ (without restoring the full $\\mathcal D$ for training after validation is done)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from week6 import Dataset, LinearClassifier, add_bias\n",
      "\n",
      "\n",
      "# define the transformations on (x1, x2)\n",
      "def phi0(X):\n",
      "  return np.ones(X.shape[0])\n",
      "\n",
      "\n",
      "def phi1(X):\n",
      "  return X[:, 0]\n",
      "\n",
      "\n",
      "def phi2(X):\n",
      "  return X[:, 1]\n",
      "\n",
      "\n",
      "def phi3(X):\n",
      "  return X[:, 0] ** 2\n",
      "\n",
      "\n",
      "def phi4(X):\n",
      "  return X[:, 1] ** 2\n",
      "\n",
      "\n",
      "def phi5(X):\n",
      "  return X[:, 0] * X[:, 1]\n",
      "\n",
      "\n",
      "def phi6(X):\n",
      "  return np.abs(X[:, 0] - X[:, 1])\n",
      "\n",
      "\n",
      "def phi7(X):\n",
      "  return np.abs(X[:, 0] + X[:, 1])\n",
      "\n",
      "\n",
      "def combined_transform(phis=None):\n",
      "  def transform(X):\n",
      "      if phis is not None and isinstance(phis, list):\n",
      "          return np.array([phi(X) for phi in phis]).T\n",
      "      return X\n",
      "  return transform\n",
      "\n",
      "transformations = [phi0, phi1, phi2, phi3, phi4, phi5, phi6, phi7]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read data\n",
      "training = np.loadtxt('data/in.dta')\n",
      "test = np.loadtxt('data/out.dta')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 1\n",
      "Split in.dta into training (first 25 examples) and validation (last 10 examples). Train on the 25 examples only, using the validation set of 10 examples to select between five models that apply linear regression to $\\phi_0$ through $\\phi_k$, with $k =3, 4, 5, 6, 7$. For which model is the classification error on the validation set smallest?\n",
      "\n",
      "\n",
      "- [a] k = 3\n",
      "- [b] k = 4\n",
      "- [c] k = 5\n",
      "- **[d] k = 6**\n",
      "- [e] k = 7\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare data set\n",
      "training_set = Dataset(training[:25, 0:2], training[:25, 2])\n",
      "validation_set = Dataset(training[25:, 0:2], training[25:, 2])\n",
      "test_set = Dataset(test[:, 0:2], test[:, 2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(3, 8):\n",
      "    phis = transformations[:k + 1]\n",
      "    classifier = LinearClassifier(Xtransformations=(combined_transform(phis), add_bias))\n",
      "    classifier.fit(training_set.get_X(), training_set.get_y())\n",
      "    print('model%d\\'s validation error is %f' %\n",
      "        (k, classifier.error(validation_set.get_X(), validation_set.get_y())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "model3's validation error is 0.300000\n",
        "model4's validation error is 0.500000\n",
        "model5's validation error is 0.200000\n",
        "model6's validation error is 0.000000\n",
        "model7's validation error is 0.100000\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 2\n",
      "Evaluate the out-of-sample classification error using out.dta on the 5 models to see how well the validation set predicted the best of the 5 models. For which model is the out-of-sample classification error smallest?\n",
      "\n",
      "- [a] k = 3\n",
      "- [b] k = 4\n",
      "- [c] k = 5\n",
      "- [d] k = 6\n",
      "- **[e] k = 7**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(3, 8):\n",
      "    phis = transformations[:k + 1]\n",
      "    classifier = LinearClassifier(Xtransformations=(combined_transform(phis), add_bias))\n",
      "    classifier.fit(training_set.get_X(), training_set.get_y())\n",
      "    print('model%d\\'s validation error is %f' %\n",
      "        (k, classifier.error(test_set.get_X(), test_set.get_y())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "model3's validation error is 0.420000\n",
        "model4's validation error is 0.416000\n",
        "model5's validation error is 0.188000\n",
        "model6's validation error is 0.084000\n",
        "model7's validation error is 0.072000\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 3\n",
      "Reverse the role of training and validation sets; now training with the last 10 examples and validating with the first 25 examples. For which model is the classification error on the validation set smallest?\n",
      "\n",
      "- [a] k = 3\n",
      "- [b] k = 4\n",
      "- [c] k = 5\n",
      "- **[d] k = 6**\n",
      "- [e] k = 7"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(3, 8):\n",
      "    phis = transformations[:k + 1]\n",
      "    classifier = LinearClassifier(Xtransformations=(combined_transform(phis), add_bias))\n",
      "    classifier.fit(validation_set.get_X(), validation_set.get_y())\n",
      "    print('model%d\\'s validation error is %f' %\n",
      "        (k, classifier.error(training_set.get_X(), training_set.get_y())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "model3's validation error is 0.280000\n",
        "model4's validation error is 0.360000\n",
        "model5's validation error is 0.200000\n",
        "model6's validation error is 0.080000\n",
        "model7's validation error is 0.120000\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 4\n",
      "Once again evaluate the out-of-sample classification error using out.dta on the 5 models to see how well the validation set predicted the best of the 5 models. For which model is the out-of-sample classification error smallest?\n",
      "\n",
      "- [a] k = 3\n",
      "- [b] k = 4\n",
      "- [c] k = 5\n",
      "- **[d] k = 6**\n",
      "- [e] k = 7"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(3, 8):\n",
      "    phis = transformations[:k + 1]\n",
      "    classifier = LinearClassifier(Xtransformations=(combined_transform(phis), add_bias))\n",
      "    classifier.fit(validation_set.get_X(), validation_set.get_y())\n",
      "    print('model%d\\'s validation error is %f' %\n",
      "        (k, classifier.error(test_set.get_X(), test_set.get_y())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "model3's validation error is 0.396000\n",
        "model4's validation error is 0.388000\n",
        "model5's validation error is 0.284000\n",
        "model6's validation error is 0.192000\n",
        "model7's validation error is 0.196000\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 5\n",
      "What values are closest to the out-of-sample classification error obtained for the model chosen in Problems 1,3, respectively?\n",
      "\n",
      "- [a] 0.0, 0.1\n",
      "- **[b] 0.1, 0.2**\n",
      "- [c] 0.1, 0.3\n",
      "- [d] 0.2, 0.2\n",
      "- [e] 0.2, 0.3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Validation Bias\n",
      "### Problem 6\n",
      "Let $e_1$ and $e_2$ be independent random variables, distributed uniformly over the interval [0, 1]. Let $e = min(e_1, e_2)$. The expected values of $e_1, e_2, e$ are closest to\n",
      "\n",
      "- [a] 0.5, 0.5, 0\n",
      "- [b] 0.5, 0.5, 0.1\n",
      "- [c] 0.5, 0.5, 0.25\n",
      "- **[d] 0.5, 0.5, 0.4**\n",
      "- [e] 0.5, 0.5, 0.5"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.uniform(0, 1, (1000000, 2)).min(1).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "0.33349444523959104"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a analytic way, $P(e > x) = P(e_1 > x)P(e_2 > x) = (1 - P(e_1 <= x))(1 - P(e_2 <= x)) = (1 - x)^2$. \n",
      "\n",
      "So the CDF of $e$ is $F(x) = 1 - P(e > x) = 1 - (1 - x) ^ 2$. The PDF of e is $f(x) = 2 - 2x$ and $E(e) = \\int_0^1 f(x)x\\,dx = \\frac{1}{3}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cross Validation\n",
      "### Problem 7\n",
      "You are given the data points: $(-1, 0),(\\rho, 1),(1, 0), \\rho > 0$, and a choice between two models: constant $h_0(x) = b$ and linear $h_1(x) = ax + b$. For which value of $\\rho$ would the two models be tied using leave-one-out cross-validation with the squared error measure?\n",
      "\n",
      "- [a] $\\sqrt{\\sqrt{3} + 4}$\n",
      "- [b] $\\sqrt{\\sqrt{3} - 1}$\n",
      "- **[c] $\\sqrt{9 + 4\\sqrt{6}}$**\n",
      "- [d] $\\sqrt{9 - \\sqrt{6}}$\n",
      "- [e] None of the above"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For $h_0$, we can get validation error 0.25, 1 and 0.25, corresponding to the hypothesis $h_0(x) = 0.5, h_0(x) = 0$ and $h_0(x) = 0.5$.\n",
      "For $h_1$, we can get validation error $(\\frac{2}{1 + \\rho})^2, 1$ and $(\\frac{2}{\\rho - 1})^2$, corresponding to the hypothesis $h_1(x) = \\frac{1}{\\rho + 1}(x+1), h_1(x) = 0$ and $h_1(x) = \\frac{1}{\\rho - 1}(x - 1)$.\n",
      "\n",
      "Then solve the equation $(\\frac{2}{1 + \\rho})^2+1+(\\frac{2}{\\rho - 1})^2 = 0.25+1+0.25$ and get $\\rho^2 = 9+4\\sqrt{6}$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## PLA vs. SVM\n",
      "_Notice: Quadratic Programming packages sometimes need tweaking and have numerical issues, and this is characteristic of packages you will use in practical ML situations. Your understanding of support vectors will help you get to the correct answers._\n",
      "\n",
      "\n",
      "In the following problems, we compare PLA to SVM with hard margin 1 on linearly separable data sets. For each run, you will create your own target function $f$ and data set $\\mathcal D$. Take $d = 2$ and choose a random line in the plane as your target function $f$ (do this by taking two random, uniformly distributed points on $[\u22121, 1] \u00d7 [\u22121, 1]$ and taking the line passing through them), where one side of the line maps to +1 and the other maps to \u22121. Choose the inputs x n of the data set as random points in $X = [\u22121, 1] \u00d7 [\u22121, 1]$, and evaluate the target function on each $x_n$ to get the corresponding output $y_n$. If all data points are on one side of the line, discard the run and start a new run. Start PLA with the all-zero vector and pick the misclassified point for each PLA iteration at random. Run PLA to find the final hypothesis $g$ PLA and measure the disagreement between $f$ and $g$ PLA as $P[f(x) = g_{PLA}(x)]$ (you can either calculate this exactly, or approximate it by generating a sufficiently large, separate set of points t evaluate it). Now, run SVM on the same data to find the final hypothesis $g_{SVM}$ by solving\n",
      "\n",
      "\\begin{equation}\n",
      "\\min_{w, b} \\frac{1}{2} w^Tw\n",
      "\\\\\n",
      "s.t. y_n(w^Tx_n+b) \\ge 1\n",
      "\\end{equation}\n",
      "\n",
      "using quadratic programming on the primal or the dual problem. Measure the disagreement between $f$ and $g_{SVM}$ as $P[f (x) = g_{SVM}(x)]$, and count the number of support vectors you get in each run."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from week1 import PointsDataset, PLA\n",
      "\n",
      "\n",
      "class SVM:\n",
      "    \"\"\"\n",
      "    A support vector machine object.\n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        self.w = None\n",
      "        self.b = None\n",
      "        self.sv = None  # indexes of support vectors\n",
      "        \n",
      "    def get_weight(self):\n",
      "        return self.w, self.b\n",
      "    \n",
      "    def get_support_vectors(self):\n",
      "        return self.sv\n",
      "\n",
      "    def fit(self, X, y):\n",
      "        \"\"\"\n",
      "        Fit the dataset\n",
      "        \"\"\"\n",
      "        assert y.shape[0] == X.shape[0]\n",
      "        \n",
      "        y = y.reshape((-1, 1))\n",
      "        n = X.shape[0]\n",
      "        d = X.shape[1]\n",
      "        Q = (y * X).dot((y * X).T)\n",
      "        p = np.ones(n) * -1\n",
      "        G = np.eye(n) * -1\n",
      "        h = np.zeros(n)\n",
      "        A = y.T\n",
      "        b = np.zeros(1)\n",
      "        \n",
      "        solvers.options['show_progress'] = False\n",
      "        alpha = solvers.qp(matrix(Q, tc='d'), matrix(p), \n",
      "                           matrix(G), matrix(h), matrix(A, tc='d'), matrix(b))['x']\n",
      "        alpha = np.round(alpha).reshape(-1)  # convert to numpy array and round to 5 decimal places\n",
      "        \n",
      "        self.w = (X * y).T.dot(alpha)\n",
      "        for i, a in enumerate(alpha.reshape(-1)):\n",
      "            if a > 0:\n",
      "                self.b = (1 / y[i]) - X[i].dot(self.w)\n",
      "                break\n",
      "        else:\n",
      "            raise AssertionError('all alphas are zero.')\n",
      "        self.sv = np.where(alpha > 0)[0]\n",
      "            \n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        Predict the label.\n",
      "        \"\"\"\n",
      "        assert self.w is not None\n",
      "        assert self.w.shape[0] == X.shape[1]\n",
      "\n",
      "        return np.where(X.dot(self.w) + self.b >= 0, 1, -1)\n",
      "\n",
      "    def error(self, X, y):\n",
      "        \"\"\"\n",
      "        Return the rate of misclassification.\n",
      "        \"\"\"\n",
      "        predicted = self.predict(X)\n",
      "        return 1 - (y == predicted).sum() / predicted.size        \n",
      "    \n",
      "    def plot(self, color='green', label=None, ax=None):\n",
      "        assert self.w is not None\n",
      "\n",
      "        if ax is None:\n",
      "            fig, ax = plt.subplots()\n",
      "        if label is None:\n",
      "            label = 'h'\n",
      "\n",
      "        x = np.linspace(-1.5, 1.5, 20)\n",
      "        y = (self.w[0] * x + self.b) / (-self.w[1])\n",
      "        subindex = np.logical_and(y <= 1.5, y >= -1.5)\n",
      "        ax.plot(x[subindex], y[subindex], color=color, label=label)\n",
      "        \n",
      "fig, ax = plt.subplots()\n",
      "training_set = PointsDataset(10)\n",
      "\n",
      "svm = SVM()\n",
      "svm.fit(training_set.get_X(), training_set.get_y())\n",
      "svm.plot(ax=ax)\n",
      "training_set.plot(ax=ax, highlighted=svm.get_support_vectors())\n",
      "ax.legend(loc=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "<matplotlib.legend.Legend at 0x7f193147a518>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f19337a2cc0>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Problem 8\n",
      "For $N = 10$, repeat the above experiment for 1000 runs. How often is $g_{SVM}$ better than $g_{PLA}$ in approximating $f$? The percentage of time is closest to:\n",
      "\n",
      "- [a] 20%\n",
      "- [b] 40%\n",
      "- **[c] 60%**\n",
      "- [d] 80%\n",
      "- [e] 100%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def do_experiment(N):\n",
      "    svm = SVM()\n",
      "    pla = PLA(Xtransformations=(add_bias,))\n",
      "    training_set = PointsDataset(N)\n",
      "    check = training_set.get_y() == 1  # check if we have all data with same label\n",
      "    while check.all() or not check.any():\n",
      "        training_set = PointsDataset(N)\n",
      "        check = training_set.get_y() == 1\n",
      "    test_set = PointsDataset(10000, boundary=training_set.get_boundary())\n",
      "    svm.fit(training_set.get_X(), training_set.get_y())\n",
      "    pla.fit(training_set.get_X(), training_set.get_y())\n",
      "    return svm.error(test_set.get_X(), test_set.get_y()) < pla.error(test_set.get_X(), test_set.get_y())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 1000\n",
      "better = 0\n",
      "for i in range(n):\n",
      "    better += do_experiment(10)\n",
      "better / n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "0.60099999999999998"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 9\n",
      "For $N = 100$, repeat the above experiment for 1000 runs. How often is $g_{SVM}$ better than $_{PLA}$ in approximating $f$? The percentage of time is closest to:\n",
      "\n",
      "- [a] 10%\n",
      "- [b] 30%\n",
      "- [c] 50%\n",
      "- **[d] 70%**\n",
      "- [e] 90%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "better = 0\n",
      "for i in range(n):\n",
      "    better += do_experiment(100)\n",
      "better / n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "0.65900000000000003"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 10\n",
      "For the case $N = 100$, which of the following is the closest to the average number of support vectors of $g_{SVM}$ (averaged over the 1000 runs)?\n",
      "\n",
      "- [a] 2\n",
      "- **[b] 3**\n",
      "- [c] 5\n",
      "- [d] 10\n",
      "- [e] 20"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm = SVM()\n",
      "nsv = 0\n",
      "for i in range(n):\n",
      "    training_set = PointsDataset(100)\n",
      "    check = training_set.get_y() == 1  # check if we have all data with same label\n",
      "    while check.all() or not check.any():\n",
      "        training_set = PointsDataset(100)\n",
      "        check = training_set.get_y() == 1\n",
      "    svm.fit(training_set.get_X(), training_set.get_y())\n",
      "    nsv += svm.get_support_vectors().size\n",
      "nsv / n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "2.998"
       ]
      }
     ],
     "prompt_number": 15
    }
   ],
   "metadata": {}
  }
 ]
}