{
 "metadata": {
  "name": "",
  "signature": "sha256:10320c73240ab31d7891a03c64e6b521f48eeaaead8b529f8acb2d8bdb90bb9f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Homework 7"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "from cvxopt import matrix, solvers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Validation\n",
      "In the following problems, use the data provided in the files in.dta and out.dta for Homework # 6 .We are going to apply linear regression with a nonlinear transformation for classification (without regularization). The nonlinear transformation is given by $\\\\phi_0$ through $\\\\phi_7$ which transform $(x_1, x_2)$ into $$1, x_1 , x_2 , x_1^2 , x_2^2 , x_1x_2 , |x_1 - x_2|, |x_1 + x_2|.$$\n",
      "To illustrate how taking out points for validation affects the performance, we will consider the hypotheses trained on $\\mathcal D_{train}$ (without restoring the full $\\mathcal D$ for training after validation is done)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lib.dataset import Dataset\n",
      "from lib.linear import LinearClassifier\n",
      "from lib.transformations import add_bias\n",
      "\n",
      "\n",
      "# define the transformations on (x1, x2)\n",
      "def phi0(X):\n",
      "  return np.ones(X.shape[0])\n",
      "\n",
      "\n",
      "def phi1(X):\n",
      "  return X[:, 0]\n",
      "\n",
      "\n",
      "def phi2(X):\n",
      "  return X[:, 1]\n",
      "\n",
      "\n",
      "def phi3(X):\n",
      "  return X[:, 0] ** 2\n",
      "\n",
      "\n",
      "def phi4(X):\n",
      "  return X[:, 1] ** 2\n",
      "\n",
      "\n",
      "def phi5(X):\n",
      "  return X[:, 0] * X[:, 1]\n",
      "\n",
      "\n",
      "def phi6(X):\n",
      "  return np.abs(X[:, 0] - X[:, 1])\n",
      "\n",
      "\n",
      "def phi7(X):\n",
      "  return np.abs(X[:, 0] + X[:, 1])\n",
      "\n",
      "\n",
      "def combined_transform(phis=None):\n",
      "  def transform(X):\n",
      "      if phis is not None and isinstance(phis, list):\n",
      "          return np.array([phi(X) for phi in phis]).T\n",
      "      return X\n",
      "  return transform\n",
      "\n",
      "transformations = [phi0, phi1, phi2, phi3, phi4, phi5, phi6, phi7]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read data\n",
      "training = np.loadtxt('data/in.dta')\n",
      "test = np.loadtxt('data/out.dta')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 1\n",
      "Split in.dta into training (first 25 examples) and validation (last 10 examples). Train on the 25 examples only, using the validation set of 10 examples to select between five models that apply linear regression to $\\phi_0$ through $\\phi_k$, with $k =3, 4, 5, 6, 7$. For which model is the classification error on the validation set smallest?\n",
      "\n",
      "\n",
      "- [a] k = 3\n",
      "- [b] k = 4\n",
      "- [c] k = 5\n",
      "- **[d] k = 6**\n",
      "- [e] k = 7\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare data set\n",
      "training_set = Dataset(training[:25, 0:2], training[:25, 2])\n",
      "validation_set = Dataset(training[25:, 0:2], training[25:, 2])\n",
      "test_set = Dataset(test[:, 0:2], test[:, 2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(3, 8):\n",
      "    phis = transformations[:k + 1]\n",
      "    classifier = LinearClassifier(Xtransformations=(combined_transform(phis), add_bias))\n",
      "    classifier.fit(training_set.get_X(), training_set.get_y())\n",
      "    print('model%d\\'s validation error is %f' %\n",
      "        (k, classifier.error(validation_set.get_X(), validation_set.get_y())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "model3's validation error is 0.300000\n",
        "model4's validation error is 0.500000\n",
        "model5's validation error is 0.200000\n",
        "model6's validation error is 0.000000\n",
        "model7's validation error is 0.100000\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 2\n",
      "Evaluate the out-of-sample classification error using out.dta on the 5 models to see how well the validation set predicted the best of the 5 models. For which model is the out-of-sample classification error smallest?\n",
      "\n",
      "- [a] k = 3\n",
      "- [b] k = 4\n",
      "- [c] k = 5\n",
      "- [d] k = 6\n",
      "- **[e] k = 7**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(3, 8):\n",
      "    phis = transformations[:k + 1]\n",
      "    classifier = LinearClassifier(Xtransformations=(combined_transform(phis), add_bias))\n",
      "    classifier.fit(training_set.get_X(), training_set.get_y())\n",
      "    print('model%d\\'s validation error is %f' %\n",
      "        (k, classifier.error(test_set.get_X(), test_set.get_y())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "model3's validation error is 0.420000\n",
        "model4's validation error is 0.416000\n",
        "model5's validation error is 0.188000\n",
        "model6's validation error is 0.084000\n",
        "model7's validation error is 0.072000\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 3\n",
      "Reverse the role of training and validation sets; now training with the last 10 examples and validating with the first 25 examples. For which model is the classification error on the validation set smallest?\n",
      "\n",
      "- [a] k = 3\n",
      "- [b] k = 4\n",
      "- [c] k = 5\n",
      "- **[d] k = 6**\n",
      "- [e] k = 7"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(3, 8):\n",
      "    phis = transformations[:k + 1]\n",
      "    classifier = LinearClassifier(Xtransformations=(combined_transform(phis), add_bias))\n",
      "    classifier.fit(validation_set.get_X(), validation_set.get_y())\n",
      "    print('model%d\\'s validation error is %f' %\n",
      "        (k, classifier.error(training_set.get_X(), training_set.get_y())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "model3's validation error is 0.280000\n",
        "model4's validation error is 0.360000\n",
        "model5's validation error is 0.200000\n",
        "model6's validation error is 0.080000\n",
        "model7's validation error is 0.120000\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 4\n",
      "Once again evaluate the out-of-sample classification error using out.dta on the 5 models to see how well the validation set predicted the best of the 5 models. For which model is the out-of-sample classification error smallest?\n",
      "\n",
      "- [a] k = 3\n",
      "- [b] k = 4\n",
      "- [c] k = 5\n",
      "- **[d] k = 6**\n",
      "- [e] k = 7"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(3, 8):\n",
      "    phis = transformations[:k + 1]\n",
      "    classifier = LinearClassifier(Xtransformations=(combined_transform(phis), add_bias))\n",
      "    classifier.fit(validation_set.get_X(), validation_set.get_y())\n",
      "    print('model%d\\'s validation error is %f' %\n",
      "        (k, classifier.error(test_set.get_X(), test_set.get_y())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "model3's validation error is 0.396000\n",
        "model4's validation error is 0.388000\n",
        "model5's validation error is 0.284000\n",
        "model6's validation error is 0.192000\n",
        "model7's validation error is 0.196000\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 5\n",
      "What values are closest to the out-of-sample classification error obtained for the model chosen in Problems 1,3, respectively?\n",
      "\n",
      "- [a] 0.0, 0.1\n",
      "- **[b] 0.1, 0.2**\n",
      "- [c] 0.1, 0.3\n",
      "- [d] 0.2, 0.2\n",
      "- [e] 0.2, 0.3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Validation Bias\n",
      "### Problem 6\n",
      "Let $e_1$ and $e_2$ be independent random variables, distributed uniformly over the interval [0, 1]. Let $e = min(e_1, e_2)$. The expected values of $e_1, e_2, e$ are closest to\n",
      "\n",
      "- [a] 0.5, 0.5, 0\n",
      "- [b] 0.5, 0.5, 0.1\n",
      "- [c] 0.5, 0.5, 0.25\n",
      "- **[d] 0.5, 0.5, 0.4**\n",
      "- [e] 0.5, 0.5, 0.5"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.uniform(0, 1, (1000000, 2)).min(1).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "0.33392008220867825"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a analytic way, $P(e > x) = P(e_1 > x)P(e_2 > x) = (1 - P(e_1 <= x))(1 - P(e_2 <= x)) = (1 - x)^2$. \n",
      "\n",
      "So the CDF of $e$ is $F(x) = 1 - P(e > x) = 1 - (1 - x) ^ 2$. The PDF of e is $f(x) = 2 - 2x$ and $E(e) = \\int_0^1 f(x)x\\,dx = \\frac{1}{3}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cross Validation\n",
      "### Problem 7\n",
      "You are given the data points: $(-1, 0),(\\rho, 1),(1, 0), \\rho > 0$, and a choice between two models: constant $h_0(x) = b$ and linear $h_1(x) = ax + b$. For which value of $\\rho$ would the two models be tied using leave-one-out cross-validation with the squared error measure?\n",
      "\n",
      "- [a] $\\sqrt{\\sqrt{3} + 4}$\n",
      "- [b] $\\sqrt{\\sqrt{3} - 1}$\n",
      "- **[c] $\\sqrt{9 + 4\\sqrt{6}}$**\n",
      "- [d] $\\sqrt{9 - \\sqrt{6}}$\n",
      "- [e] None of the above"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For $h_0$, we can get validation error 0.25, 1 and 0.25, corresponding to the hypothesis $h_0(x) = 0.5, h_0(x) = 0$ and $h_0(x) = 0.5$.\n",
      "For $h_1$, we can get validation error $(\\frac{2}{1 + \\rho})^2, 1$ and $(\\frac{2}{\\rho - 1})^2$, corresponding to the hypothesis $h_1(x) = \\frac{1}{\\rho + 1}(x+1), h_1(x) = 0$ and $h_1(x) = \\frac{1}{\\rho - 1}(x - 1)$.\n",
      "\n",
      "Then solve the equation $(\\frac{2}{1 + \\rho})^2+1+(\\frac{2}{\\rho - 1})^2 = 0.25+1+0.25$ and get $\\rho^2 = 9+4\\sqrt{6}$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## PLA vs. SVM\n",
      "_Notice: Quadratic Programming packages sometimes need tweaking and have numerical issues, and this is characteristic of packages you will use in practical ML situations. Your understanding of support vectors will help you get to the correct answers._\n",
      "\n",
      "\n",
      "In the following problems, we compare PLA to SVM with hard margin 1 on linearly separable data sets. For each run, you will create your own target function $f$ and data set $\\mathcal D$. Take $d = 2$ and choose a random line in the plane as your target function $f$ (do this by taking two random, uniformly distributed points on $[\u22121, 1] \u00d7 [\u22121, 1]$ and taking the line passing through them), where one side of the line maps to +1 and the other maps to \u22121. Choose the inputs x n of the data set as random points in $X = [\u22121, 1] \u00d7 [\u22121, 1]$, and evaluate the target function on each $x_n$ to get the corresponding output $y_n$. If all data points are on one side of the line, discard the run and start a new run. Start PLA with the all-zero vector and pick the misclassified point for each PLA iteration at random. Run PLA to find the final hypothesis $g$ PLA and measure the disagreement between $f$ and $g$ PLA as $P[f(x) = g_{PLA}(x)]$ (you can either calculate this exactly, or approximate it by generating a sufficiently large, separate set of points t evaluate it). Now, run SVM on the same data to find the final hypothesis $g_{SVM}$ by solving\n",
      "\n",
      "\\begin{equation}\n",
      "\\min_{w, b} \\frac{1}{2} w^Tw\n",
      "\\\\\n",
      "s.t. y_n(w^Tx_n+b) \\ge 1\n",
      "\\end{equation}\n",
      "\n",
      "using quadratic programming on the primal or the dual problem. Measure the disagreement between $f$ and $g_{SVM}$ as $P[f (x) = g_{SVM}(x)]$, and count the number of support vectors you get in each run."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lib.dataset import PointsDataset\n",
      "from lib.pla import PLA\n",
      "\n",
      "\n",
      "# Kernel functions\n",
      "# All operations are performed on third dimention which is original feature dimention.\n",
      "# All kernel functions receive two array-like paramters which have shape (n1, d) and (n2, d)\n",
      "# and return one array-like result which has shape (n1, n2)\n",
      "def linear_kernel():\n",
      "    \"\"\"\n",
      "    form: <x, x_prime>\n",
      "    \"\"\"\n",
      "    def f(x, x_prime):\n",
      "        if x.ndim == 1:\n",
      "            x = x[np.newaxis, :]\n",
      "        if x_prime.ndim == 1:\n",
      "            x_prime = x_prime[np.newaxis, :]\n",
      "        assert x.shape[1] == x_prime.shape[1]  # must have the same dimention\n",
      "\n",
      "        shape = (x.shape[0], x_prime.shape[0])\n",
      "        x = x[:, np.newaxis, :]\n",
      "        x_prime = x_prime[np.newaxis, :, :]\n",
      "        return np.inner(x, x_prime).reshape(shape)\n",
      "    return f\n",
      "\n",
      "\n",
      "def poly_kernel(degree, gamma=1, coef=1):\n",
      "    \"\"\"\n",
      "    form: (coef + gamma * <x, x_prime>) ** degree\n",
      "    \"\"\"\n",
      "    def f(x, x_prime):\n",
      "        if x.ndim == 1:\n",
      "            x = x[np.newaxis, :]\n",
      "        if x_prime.ndim == 1:\n",
      "            x_prime = x_prime[np.newaxis, :]\n",
      "        assert x.shape[1] == x_prime.shape[1]  # must have the same dimention\n",
      "\n",
      "        shape = (x.shape[0], x_prime.shape[0])\n",
      "        x = x[:, np.newaxis, :]\n",
      "        x_prime = x_prime[np.newaxis, :, :]\n",
      "        k = (np.inner(x, x_prime) * gamma + coef) ** degree\n",
      "        return k.reshape(shape)\n",
      "    return f\n",
      "\n",
      "\n",
      "def rbf_kernel(gamma):\n",
      "    \"\"\"\n",
      "    form: exp(-gamma * <x, x_prime>) ** 2\n",
      "    \"\"\"\n",
      "    def f(x, x_prime):\n",
      "        if x.ndim == 1:\n",
      "            x = x[np.newaxis, :]\n",
      "        if x_prime.ndim == 1:\n",
      "            x_prime = x_prime[np.newaxis, :]\n",
      "        assert x.shape[1] == x_prime.shape[1]  # must have the same dimention\n",
      "\n",
      "        x = x[:, np.newaxis, :]\n",
      "        x_prime = x_prime[np.newaxis, :, :]\n",
      "        k = np.exp(-1 * gamma * ((x - x_prime) ** 2).sum(axis=2))\n",
      "        return k\n",
      "    return f\n",
      "\n",
      "\n",
      "class SVM:\n",
      "    \"\"\"\n",
      "    A support vector machine object.\n",
      "    \"\"\"\n",
      "    def __init__(self, tol=1e-3, kernel=None):\n",
      "        \"\"\"\n",
      "        tol: Tolerance that is criterion of determining wether an alpha is zero or not.\n",
      "        Try small value for stochastic kernels.\n",
      "        For example, when use rbf kernel, tol=1e6 may be a better choice than default.\n",
      "        \"\"\"\n",
      "        if kernel is None:\n",
      "            kernel = linear_kernel()\n",
      "\n",
      "        self.tol = tol\n",
      "        self.kernel = kernel  # kernel function\n",
      "        self.alphas, self.b = None, None\n",
      "        self._X, self._y = None, None  # supports vectors\n",
      "        self.supports = None  # index of support vectors\n",
      "\n",
      "    def get_weight(self):\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def get_supports(self):\n",
      "        \"\"\"\n",
      "        Return the index of support vectors\n",
      "        \"\"\"\n",
      "        return self.supports\n",
      "\n",
      "    def get_support_vectors(self):\n",
      "        \"\"\"\n",
      "        Return support vectors\n",
      "        \"\"\"\n",
      "        return self._X\n",
      "\n",
      "    def _estimate(self, x):\n",
      "        \"\"\"\n",
      "        Compute the value of alpha * y * kernel\n",
      "        \"\"\"\n",
      "        assert self.alphas is not None and self.supports is not None\n",
      "\n",
      "        return ((self.alphas * self._y)[:, np.newaxis]\n",
      "                * self.kernel(self._X, x)).sum(axis=0)\n",
      "\n",
      "    def fit(self, X, y):\n",
      "        \"\"\"\n",
      "        Fit the dataset\n",
      "        \"\"\"\n",
      "        assert y.shape[0] == X.shape[0]\n",
      "\n",
      "        y = y.reshape((-1, 1))  # force column vector\n",
      "        n, d = X.shape\n",
      "\n",
      "        Q = y.dot(y.T) * self.kernel(X, X)  # now much faster\n",
      "        p = np.ones(n) * -1\n",
      "        # alpha >= 0\n",
      "        G = np.eye(n) * -1\n",
      "        h = np.zeros(n)\n",
      "        # alpha * y = 0\n",
      "        A = y.T\n",
      "        b = np.zeros(1)\n",
      "\n",
      "        solvers.options['show_progress'] = False\n",
      "        alphas = solvers.qp(matrix(Q, tc='d'), matrix(p), matrix(G),\n",
      "                            matrix(h), matrix(A, tc='d'), matrix(b))['x']\n",
      "\n",
      "        alphas = np.ravel(alphas)\n",
      "        if np.all(alphas < self.tol):\n",
      "            raise AssertionError('all alphas are zero.')\n",
      "        self.supports = np.where(alphas > self.tol)[0]\n",
      "        self.alphas = alphas[self.supports]\n",
      "        self._X, self._y = X[self.supports], y[self.supports].ravel()\n",
      "        m = self.supports[0]\n",
      "        self.b = y[m] - self._estimate(X[m])\n",
      "\n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        Predict the label.\n",
      "        \"\"\"\n",
      "        return np.where(self._estimate(X) + self.b >= 0, 1, -1)\n",
      "\n",
      "    def error(self, X, y):\n",
      "        \"\"\"\n",
      "        Return the rate of misclassification.\n",
      "        \"\"\"\n",
      "        predicted = self.predict(X)\n",
      "        return 1 - (y == predicted).sum() / predicted.size\n",
      "\n",
      "    def plot(self, X, y, ax=None):\n",
      "        assert self.alphas is not None and self.supports is not None\n",
      "\n",
      "        if ax is None:\n",
      "            fig, ax = plt.subplots()\n",
      "\n",
      "        # create a mesh to plot in\n",
      "        h = .02  # step size\n",
      "        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
      "        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
      "        x1x1, x2x2 = np.meshgrid(np.arange(x1_min, x1_max, h),\n",
      "                                 np.arange(x2_min, x2_max, h))\n",
      "\n",
      "        # plot boundary\n",
      "        labels = self.predict(np.c_[x1x1.ravel(), x2x2.ravel()])\n",
      "        ax.contourf(x1x1, x2x2, labels.reshape(x1x1.shape),\n",
      "                    cmap=plt.cm.Paired, alpha=0.8)\n",
      "        # plot data set\n",
      "        plt.scatter(X[:, 0], X[:, 1], s=40, c=y, cmap=plt.cm.Paired)\n",
      "        # plot support vectors\n",
      "        # draw a square around each support vector\n",
      "        square = {'marker': 's',\n",
      "                  'facecolor': 'none',\n",
      "                  'edgecolor': 'black',\n",
      "                  's': 100}\n",
      "        plt.scatter(self.get_support_vectors()[:, 0],\n",
      "                    self.get_support_vectors()[:, 1], **square)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "training_set = PointsDataset(10)\n",
      "X, y = training_set.get_X(), training_set.get_y()\n",
      "svm = SVM()\n",
      "svm.fit(X, y)\n",
      "svm.plot(X, y, ax=ax)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1 4 6]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD7CAYAAACRxdTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSASp3EVEIkjlLiISQSp3EZEI+n8iWovKWz40NwAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f070e9ac390>"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Problem 8\n",
      "For $N = 10$, repeat the above experiment for 1000 runs. How often is $g_{SVM}$ better than $g_{PLA}$ in approximating $f$? The percentage of time is closest to:\n",
      "\n",
      "- [a] 20%\n",
      "- [b] 40%\n",
      "- **[c] 60%**\n",
      "- [d] 80%\n",
      "- [e] 100%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def do_experiment(N):\n",
      "    svm = SVM()\n",
      "    pla = PLA(Xtransformations=(add_bias,))\n",
      "    training_set = PointsDataset(N)\n",
      "    check = training_set.get_y() == 1  # check if we have all data with same label\n",
      "    while check.all() or not check.any():\n",
      "        training_set = PointsDataset(N)\n",
      "        check = training_set.get_y() == 1\n",
      "    test_set = PointsDataset(10000, boundary=training_set.get_boundary())\n",
      "    svm.fit(training_set.get_X(), training_set.get_y())\n",
      "    pla.fit(training_set.get_X(), training_set.get_y())\n",
      "    return svm.error(test_set.get_X(), test_set.get_y()) < pla.error(test_set.get_X(), test_set.get_y())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 1000\n",
      "better = 0\n",
      "for i in range(n):\n",
      "    better += do_experiment(10)\n",
      "better / n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "0.61199999999999999"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 9\n",
      "For $N = 100$, repeat the above experiment for 1000 runs. How often is $g_{SVM}$ better than $_{PLA}$ in approximating $f$? The percentage of time is closest to:\n",
      "\n",
      "- [a] 10%\n",
      "- [b] 30%\n",
      "- [c] 50%\n",
      "- **[d] 70%**\n",
      "- [e] 90%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "better = 0\n",
      "for i in range(n):\n",
      "    better += do_experiment(100)\n",
      "better / n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "0.68799999999999994"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 10\n",
      "For the case $N = 100$, which of the following is the closest to the average number of support vectors of $g_{SVM}$ (averaged over the 1000 runs)?\n",
      "\n",
      "- [a] 2\n",
      "- **[b] 3**\n",
      "- [c] 5\n",
      "- [d] 10\n",
      "- [e] 20"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm = SVM()\n",
      "nsv = 0  # number of support vectors\n",
      "for i in range(n):\n",
      "    training_set = PointsDataset(100)\n",
      "    check = training_set.get_y() == 1  # check if we have all data with same label\n",
      "    while check.all() or not check.any():\n",
      "        training_set = PointsDataset(100)\n",
      "        check = training_set.get_y() == 1\n",
      "    svm.fit(training_set.get_X(), training_set.get_y())\n",
      "    nsv += svm.get_supports().size\n",
      "nsv / n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "3.006"
       ]
      }
     ],
     "prompt_number": 22
    }
   ],
   "metadata": {}
  }
 ]
}