{
 "metadata": {
  "name": "",
  "signature": "sha256:85f2839e495c2e931ae0047f582612ebfe44a054c8399aac4222bfb982175333"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Homework 6"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Overfitting and Deterministic Noise\n",
      "### Problem 1\n",
      "\n",
      "Deterministic noise depends on $H$, as some models approximate $f$ better than others. Assume $H^{\\prime} \\subset H$ and that $f$ is fixed. In general (but not necessarily in all cases), if we use $H^\\prime$ instead of $H$, how does deterministic noise behave?\n",
      "\n",
      "- [a] In general, deterministic noise will decrease.\n",
      "- **[b] In general, deterministic noise will increase.**\n",
      "- [c] In general, deterministic noise will be the same.\n",
      "- [d] There is deterministic noise for only one of $H^\\prime$ and $H$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Answer:** Because $H^\\prime \\subset H$, $H^\\prime$ has less capability to approximate $f$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Overfitting and Regularization with Weight Decay\n",
      "In the following problems use the data provided in the files\n",
      "\n",
      "http://work.caltech.edu/data/in.dta\n",
      "\n",
      "http://work.caltech.edu/data/out.dta\n",
      "\n",
      "as a training and test set respectively. Each line of the files corresponds to a two-\n",
      "dimensional input $x = (x_1 , x_2)$, so that $X = \\mathcal R_2$ , followed by the corresponding\n",
      "label from $Y = \\{\u22121, 1\\}$. We are going to apply Linear Regression with a non-linear\n",
      "transformation for classification. The nonlinear transformation is given by\n",
      "$$\\phi(x_1 , x_2) = (1, x_1 , x_2 , x_1^2 , x_2^2 , x_1x_2 , |x_1 \u2212 x_2|, |x_1 + x_2|).$$\n",
      "Recall that the classification error is defined as the fraction of miss-classified points."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lib.dataset import Dataset\n",
      "# read data\n",
      "training = np.loadtxt('data/in.dta')\n",
      "test = np.loadtxt('data/out.dta')\n",
      "\n",
      "# prepare data set\n",
      "training_set = Dataset(training[:, 0:2], training[:, 2])\n",
      "test_set = Dataset(test[:, 0:2], test[:, 2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lib.pla import PLA\n",
      "from lib.transformations import add_bias\n",
      "\n",
      "\n",
      "class LinearRegression(PLA):\n",
      "    \"\"\"\n",
      "    A linear regression object.\n",
      "    \"\"\"\n",
      "    def __init__(self, lambd=0, Xtransformations=None, ytransformations=None):\n",
      "        \"\"\"\n",
      "        transform_X: a list of transformations applied to X in given order.\n",
      "        transfrom_y: a list of transformations applied to y in given order.\n",
      "        \"\"\"\n",
      "        self.lambd = lambd  # regularzation coefficient\n",
      "        self.w = None  # weight\n",
      "        super().__init__(Xtransformations, ytransformations)\n",
      "\n",
      "    def fit(self, X, y):\n",
      "        \"\"\"\n",
      "        Fit the dataset.\n",
      "        \"\"\"\n",
      "        X = self.transformX(X)\n",
      "        y = self.transformy(y)\n",
      "        assert y.shape[0] == X.shape[0]\n",
      "\n",
      "        d = X.shape[1]\n",
      "        self.w = np.linalg.pinv(X.T.dot(X) + self.lambd * np.eye(d)).dot(X.T).dot(y)\n",
      "\n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        Predict the value.\n",
      "        \"\"\"\n",
      "        X = self.transformX(X)\n",
      "        assert self.w is not None\n",
      "        assert self.w.shape[0] == X.shape[1]\n",
      "\n",
      "        predicted = X.dot(self.w)\n",
      "        return predicted\n",
      "\n",
      "    def error(self, X, y):\n",
      "        \"\"\"\n",
      "        Return RMS error.\n",
      "        \"\"\"\n",
      "        predicted = self.predict(X)\n",
      "        y = self.transformy(y)\n",
      "        d = y - predicted\n",
      "        return np.sqrt(d.dot(d) / d.size)\n",
      "\n",
      "\n",
      "class LinearClassifier(LinearRegression):\n",
      "    \"\"\"\n",
      "    A binary classifier using linear regression.\n",
      "    y in {-1, 1}\n",
      "    \"\"\"\n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        Predict the label of the given dataset.\n",
      "        \"\"\"\n",
      "        predicted = super().predict(X)\n",
      "        predicted[predicted >= 0] = 1\n",
      "        predicted[predicted < 0] = -1\n",
      "        return predicted\n",
      "\n",
      "    def error(self, X, y):\n",
      "        \"\"\"\n",
      "        Return error rate as the fraction of misclassified data.\n",
      "        \"\"\"\n",
      "        predicted = self.predict(X)\n",
      "        y = self.transformy(y)\n",
      "        return 1 - (y == predicted).sum() / predicted.size \n",
      "\n",
      "\n",
      "def phi_transform(X):\n",
      "    \"\"\"\n",
      "    Phi transform.(Defined above)\n",
      "    \"\"\"\n",
      "    x1 = X[:, 0]\n",
      "    x2 = X[:, 1]\n",
      "    N = x1.size\n",
      "    return np.vstack([np.ones(N),\n",
      "                      x1,\n",
      "                      x2,\n",
      "                      x1**2,\n",
      "                      x2**2,\n",
      "                      x1*x2,\n",
      "                      np.abs(x1 - x2),\n",
      "                      np.abs(x1 + x2)]).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 2\n",
      "Run Linear Regression on the training set after performing the non-linear transformation. What values are closest (in Euclidean distance) to the insample andout-of-sample classification errors, respectively?\n",
      "\n",
      "- **[a] 0.03, 0.08**\n",
      "- [b] 0.03, 0.10\n",
      "- [c] 0.04, 0.09\n",
      "- [d] 0.04, 0.11\n",
      "- [e] 0.05, 0.10"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = LinearClassifier(Xtransformations=(phi_transform, add_bias))\n",
      "classifier.fit(training_set.get_X(), training_set.get_y())\n",
      "print('training error %f, test error %f' \n",
      "      % (classifier.error(training_set.get_X(), training_set.get_y()), \n",
      "         classifier.error(test_set.get_X(), test_set.get_y())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "training error 0.028571, test error 0.084000\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 3\n",
      "Now add weight decay to Linear Regression, that is, add the term $\\frac{\\lambda}{N} \\sum_{i=0}^7 w_i^2$ to the squared in-sample error, using $\\lambda = 10^k$. What are the closest values to the in-sample and out-of-sample classification errors, respectively, for $k = \u22123$?Recall that the solution for Linear Regression with Weight Decay was derived in class.\n",
      "\n",
      "- [a] 0.01, 0.02\n",
      "- [b] 0.02, 0.04\n",
      "- [c] 0.02, 0.06\n",
      "- **[d] 0.03, 0.08**\n",
      "- [e] 0.03, 0.10"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = -3\n",
      "classifier = LinearClassifier(lambd=10**k, Xtransformations=(phi_transform, add_bias))\n",
      "classifier.fit(training_set.get_X(), training_set.get_y())\n",
      "print('training error %f, test error %f' \n",
      "      % (classifier.error(training_set.get_X(), training_set.get_y()), \n",
      "         classifier.error(test_set.get_X(), test_set.get_y())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "training error 0.028571, test error 0.080000\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 4\n",
      "Now, use $k = 3$. What are the closest values to the new in-sample and out-of-sample classification errors, respectively?\n",
      "\n",
      "- [a] 0.2, 0.2\n",
      "- [b] 0.2, 0.3\n",
      "- [c] 0.3, 0.3\n",
      "- [d] 0.3, 0.4\n",
      "- **[e] 0.4, 0.4**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = 3\n",
      "classifier = LinearClassifier(lambd=10**k, Xtransformations=(phi_transform, add_bias))\n",
      "classifier.fit(training_set.get_X(), training_set.get_y())\n",
      "print('training error %f, test error %f' \n",
      "      % (classifier.error(training_set.get_X(), training_set.get_y()), \n",
      "         classifier.error(test_set.get_X(), test_set.get_y())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "training error 0.428571, test error 0.472000\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 5\n",
      "What value of $k$, among the following choices, achieves the smallest out-of-sample classification error?\n",
      "\n",
      "- [a] 2\n",
      "- [b] 1\n",
      "- [c] 0\n",
      "- **[d] -1**\n",
      "- [e] -2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(-2, 3):\n",
      "    classifier = LinearClassifier(lambd=10**k, Xtransformations=(phi_transform, add_bias))\n",
      "    classifier.fit(training_set.get_X(), training_set.get_y())\n",
      "    print('training error %f, test error %f' \n",
      "          % (classifier.error(training_set.get_X(), training_set.get_y()), \n",
      "             classifier.error(test_set.get_X(), test_set.get_y())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "training error 0.028571, test error 0.080000\n",
        "training error 0.028571, test error 0.056000\n",
        "training error 0.000000, test error 0.088000\n",
        "training error 0.057143, test error 0.116000\n",
        "training error 0.257143, test error 0.264000\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 6\n",
      "What value is closest to the minimum out-of-sample classification error achieved by varying $k$ (limiting $k$ to integer values)?\n",
      "\n",
      "- [a] 0.04\n",
      "- **[b] 0.06**\n",
      "- [c] 0.08\n",
      "- [d] 0.10\n",
      "- [e] 0.12"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Answer:** According to the result of problem 5, when $k = -1$, out-of-sample error achieve the minimum(0.56)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Neural Networks\n",
      "### Problem 8\n",
      "A fully connected Neural Network has $L = 2; d^{(0)} = 5, d^{(1)} = 3, d^{(2)} = 1.$ If only products of the form $w^{(l)}_{ij} x^{(l-1)}_i, w^{(l)}_{ij} \\delta^{(l)}_j,$ and $x^{(l-1)}_i \\delta^{(l)}_j$ count as operations(even for $x^{(l)}_0 = 1$), without counting anything else, which of the following is the closest to the total number of operations required in a single iteration of backpropagation (using SGD on one data point)?\n",
      "\n",
      "- [a] 30\n",
      "- [b] 35\n",
      "- [c] 40\n",
      "- **[d] 45**\n",
      "- [e] 50"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- forward\n",
      "    1. first layer. 6 * 3 = 18 $w^{(l)}_{ij} x^{(l-1)}_i$ operations.\n",
      "    2. second layer. 4 * 1 = 4 $w^{(l)}_{ij} x^{(l-1)}_i$ operations.\n",
      "- backpropagation\n",
      "    1. second layer.\n",
      "        + compute $\\delta^{(l-1)}$. 3 * 1 = 3 $w^{(l)}_{ij} \\delta^{(l)}_j$ operations.\n",
      "        + update $w^{(l)}$. 4 * 1 = 4 $x^{(l-1)}_i \\delta^{(l)}_j$ operations.\n",
      "    2. first layer.\n",
      "        + update $w^{(l)}$. 6 * 3 = 18  $x^{(l-1)}_i \\delta^{(l)}_j$ operations.\n",
      "        \n",
      "total $18 + 4 + 3 +  4 + 18 = 47$ operations."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}