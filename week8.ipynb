{
 "metadata": {
  "author": "nanaya",
  "name": "",
  "signature": "sha256:894fddf033076f2f0848f94c663bfddd5b1778d7c888bb757f7a107f50b2efeb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Howework 8"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Primal versus Dual Problem\n",
      "### Problem 1\n",
      "Recall that $N$ is the size of the data set and $d$ is the dimensionality of the input space. The original formulation of the hard-margin SVM problem (minimize $\\frac{1}{2} w^T w$ subject to the inequality constraints), without going through the\n",
      "Lagrangian dual problem, is\n",
      "\n",
      "- [a] a quadratic programming problem with N variables\n",
      "- [b] a quadratic programming problem with N + 1 variables\n",
      "- [c] a quadratic programming problem with d variables\n",
      "- **[d] a quadratic programming problem with d + 1 variables**\n",
      "- [e] not a quadratic programming problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The prime problem is to minimize $\\frac{1}{2} w^Tw$ subject to $y_n(w^Tx_n + b)$ for $(1,2,...,N)$.\n",
      "\n",
      "The corresponding quadratic problem is to minimize $\\frac{1}{2} u^TQu + P^Tu$ subject to $Au \\ge 1$,\n",
      "\n",
      "where\n",
      "$$ u = \\left[ \\begin{array}{c} b \\\\ w \\end{array} \\right]$$\n",
      "$$ Q = \\left[ \\begin{array}{cc} 0 & \\mathbf 0_d \\\\ \\mathbf 0_d & I_d \\end{array} \\right]$$\n",
      "$$ P = \\left[ \\begin{array}{c} \\mathbf 0_{d+1} \\end{array} \\right] $$\n",
      "$$ A = \\left[ \\begin{array}{cc} y_1 & -y_1\\mathbf x_1^T- \\\\ \n",
      "\\vdots & \\vdots \\\\ \n",
      "y_n & -y_n\\mathbf x_n^T- \n",
      "\\end{array} \\right] $$\n",
      "$$ c = \\left[ \\begin{array}{c} \\mathbf 1_{N} \\end{array} \\right] $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_Notice: The following problems deal with a real-life data set. In addition, the computational packages you use may employ different heuristics and require different tweaks. This is a typical situation that a Machine Learning practitioner faces. There are uncertainties, and the answers may or may not match our expectations. Although this situation is not as \u2018sanitized\u2019 as other homework problems, it is important to go through it as part of the learning experience._"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Support Vector Machines With Soft Margins\n",
      "In this homework set, we are going to experiment with a real-world dataset. Download the processed US Postal Service Zip Code dataset with extracted features of symmetry and intensity for training and testing:\n",
      "\n",
      "http://www.amlbook.com/data/zip/features.train\n",
      "\n",
      "http://www.amlbook.com/data/zip/features.test\n",
      "\n",
      "(the format of each row is: digit symmetry intensity ). We will train two types of binary classifiers; one-versus-one (one digit is class +1 and another digit is class \u22121, with the rest of the digits disregarded), and one-versus-all (one digit is class +1 and the rest of the digits are class \u22121).\n",
      "The data set has thousands of points, and some quadratic programming packages cannot handle this size. \n",
      "\n",
      "We recommend that you use the packages in libsvm:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implement SVM with soft margin on the above zip-code data set by solving\n",
      "\\begin{align*}\n",
      "\\min_{\\alpha} & \\frac{1}{2}\\sum_{n=1}^N \\ \\sum_{m=1}^N \n",
      "                \\alpha_n\\alpha_m y_ny_m K(\\mathbf x_n, \\mathbf x_m) - \\sum_{n=1}^N \\alpha_n \\\\\n",
      "s.t. & \\sum_{n=1}^N y_n\\alpha_n = 0 \\\\ \n",
      "& 0 \\le \\alpha_n \\le C n = 1, 2,..., N\n",
      "\\end{align*}\n",
      "\n",
      "\n",
      "When evaluating $E_{in}$ and $E_{out}$ of the resulting classifier, use binary classification error.\n",
      "Practical remarks:\n",
      "\n",
      "1. For the purpose of this homework, do not scale the data when you use libsvm or other packages, lest you should change the effective kernel and get different results.\n",
      "2. In some packages, you need to specify double precision.\n",
      "3. In 10-fold cross validation, if the data size is not a multiple of 10, the sizes of the 10 subsets may be off by 1 data point.\n",
      "4. Some packages have software parameters whose values affect the outcome. ML practitioners have to deal with this kind of added uncertainty."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Polynomial Kernels\n",
      "\n",
      "Consider the polynomial kernel $K(x_n, x_m ) = (1 + x^T_n x_m)^Q$ , where $Q$ is the degree of the polynomial."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 1\n",
      "With $C = 0.01$ and $Q = 2$, which of the following classifiers has the highest $E_{in}$ ?\n",
      "\n",
      "- **[a] 0 versus all**\n",
      "- [b] 2 versus all\n",
      "- [c] 4 versus all\n",
      "- [d] 6 versus all\n",
      "- [e] 8 versus all"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "curl http://www.amlbook.com/data/zip/features.train -o data/features.train\n",
      "curl http://www.amlbook.com/data/zip/features.test -o data/features.test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
        "                                 Dload  Upload   Total   Spent    Left  Speed\n",
        "\r",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
        "  8  348k    8 30719    0     0  27325      0  0:00:13  0:00:01  0:00:12 49308\r",
        " 74  348k   74  261k    0     0   119k      0  0:00:02  0:00:02 --:--:--  154k\r",
        " 94  348k   94  329k    0     0   104k      0  0:00:03  0:00:03 --:--:--  124k\r",
        "100  348k  100  348k    0     0  98556      0  0:00:03  0:00:03 --:--:--  111k\n",
        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
        "                                 Dload  Upload   Total   Spent    Left  Speed\n",
        "\r",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
        " 28 98343   28 28144    0     0  50286      0  0:00:01 --:--:--  0:00:01 59375\r",
        "100 98343  100 98343    0     0   139k      0 --:--:-- --:--:-- --:--:--  159k\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training = np.loadtxt('data/features.train')\n",
      "test = np.loadtxt('data/features.test')\n",
      "training[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "array([[ 6.        ,  0.3410918 , -4.5289375 ],\n",
        "       [ 5.        ,  0.44413086, -5.4968125 ],\n",
        "       [ 4.        ,  0.23100195, -2.88675   ],\n",
        "       [ 7.        ,  0.20027539, -3.534375  ],\n",
        "       [ 3.        ,  0.29193555, -4.3520625 ],\n",
        "       [ 6.        ,  0.21278516, -3.721375  ],\n",
        "       [ 3.        ,  0.20337891, -3.3363125 ],\n",
        "       [ 1.        ,  0.12304297, -0.707875  ],\n",
        "       [ 0.        ,  0.28954102, -4.644125  ],\n",
        "       [ 1.        ,  0.11385937, -0.931375  ]])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "\n",
      "# training set\n",
      "X = training[:, 1:]\n",
      "y = training[:, 0]\n",
      "# test set\n",
      "tX = test[:, 1:]\n",
      "ty = test[:, 0]\n",
      "\n",
      "C, Q = 0.01, 2\n",
      "classifier = SVC(kernel='poly', C=C, degree=Q, shrinking=False, gamma=1, coef0=1)\n",
      "print('i\\te_in\\te_out')\n",
      "for i in range(10):\n",
      "    classifier.fit(X, y == i)\n",
      "    print('%d\\t%.4f\\t%.4f' %\n",
      "          (i, 1 - classifier.score(X, y == i), 1 - classifier.score(tX, ty == i)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i\te_in\te_out\n",
        "0\t0.1059\t0.1116"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1\t0.0144\t0.0219"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2\t0.1003\t0.0987"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3\t0.0902\t0.0827"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4\t0.0894\t0.0997"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5\t0.0763\t0.0797"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6\t0.0911\t0.0847"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7\t0.0885\t0.0732"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8\t0.0743\t0.0827"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9\t0.0883\t0.0882"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 3\n",
      "With $C = 0.01$ and $Q = 2$, which of the following classifiers has the lowest $E_{in}$?\n",
      "\n",
      "- **[a] 1 versus all**\n",
      "- [b] 3 versus all\n",
      "- [c] 5 versus all\n",
      "- [d] 7 versus all\n",
      "- [e] 9 versus all"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 4\n",
      "Comparing the two selected classifiers from Problems 2 and 3, which of the following values is the closest to the difference between the number of support vectors of these two classifiers?\n",
      "\n",
      "- [a] 600\n",
      "- [b] 1200\n",
      "- **[c] 1800**\n",
      "- [d] 2400\n",
      "- [e] 3000"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf0 = SVC(kernel='poly', C=C, degree=Q, shrinking=False, gamma=1, coef0=1)\n",
      "clf1 = SVC(kernel='poly', C=C, degree=Q, shrinking=False, gamma=1, coef0=1)\n",
      "clf0.fit(X, y == 0)\n",
      "clf1.fit(X, y == 1)\n",
      "nsv0 = np.sum(clf0.n_support_)\n",
      "nsv1 = np.sum(clf1.n_support_)\n",
      "print('Total number of data is %d.\\n'\n",
      "      'The number of support vectors of clf0 is %d.\\n'\n",
      "      'The number of support vectors of clf1 is %d.\\n'\n",
      "      'The difference is %d.'\n",
      "      % (training.shape[0], nsv0, nsv1, nsv0 - nsv1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total number of data is 7291.\n",
        "The number of support vectors of clf0 is 2179.\n",
        "The number of support vectors of clf1 is 386.\n",
        "The difference is 1793.\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 5\n",
      "Consider the 1 versus 5 classifier with $Q = 2$ and $C \\in \\{0.001, 0.01, 0.1, 1\\}$. Which of the following statements is correct? Going up or down means strictly so.\n",
      "\n",
      "- [a] The number of support vectors goes down when $C$ goes up\n",
      "- [b] The number of support vectors goes up when $C$ goes up\n",
      "- [c] $E_{out}$ goes down when $C$ goes up\n",
      "- **[d] Maximum $C$ achieves the lowest $E_{in}$**\n",
      "- [e] None of the above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_y(values):\n",
      "    \"\"\"\n",
      "    Select y of which the value is in given values list.\n",
      "    \"\"\"\n",
      "    def f(y):\n",
      "        subset = False\n",
      "        for v in values:\n",
      "            subset = np.logical_or(subset, np.equal(y, v))\n",
      "        return subset\n",
      "    return f\n",
      "\n",
      "\n",
      "def subset(X, y, filter_X=None, filter_y=None):\n",
      "    \"\"\"\n",
      "    Subset data set.\n",
      "    \"\"\"\n",
      "    _X = X\n",
      "    _y = y\n",
      "    if filter_X is not None:\n",
      "        filter = filter_X(_X)\n",
      "    if filter_y is not None:\n",
      "        filter = filter_y(_y)\n",
      "    X = _X[filter]\n",
      "    y = _y[filter]\n",
      "    return X, y\n",
      "\n",
      "# create a subset of number 1 and number 5\n",
      "select_1_5 = filter_y((1, 5))\n",
      "subset_X, subset_y = subset(X, y, filter_y=select_1_5)\n",
      "subset_tX, subset_ty = subset(tX, ty, filter_y=select_1_5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C, Q = 0.0001, 2\n",
      "\n",
      "print('C\\te_in\\te_out\\tnsv')\n",
      "while C <= 1:\n",
      "    classifier = SVC(kernel='poly', C=C, degree=Q, shrinking=False, gamma=1, coef0=1)\n",
      "    classifier.fit(subset_X, subset_y)\n",
      "    print('%.4f\\t%.4f\\t%.4f\\t%d' \n",
      "          % (C, 1 - classifier.score(subset_X, subset_y), \n",
      "             1 - classifier.score(subset_tX, subset_ty), np.sum(classifier.n_support_)))\n",
      "    C *= 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C\te_in\te_out\tnsv\n",
        "0.0001\t0.0090\t0.0165\t236\n",
        "0.0010\t0.0045\t0.0165\t76\n",
        "0.0100\t0.0045\t0.0189\t34\n",
        "0.1000\t0.0045\t0.0189\t24\n",
        "1.0000\t0.0032\t0.0189\t24\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 6\n",
      "In the 1 versus 5 classifier, comparing $Q = 2$ with $Q = 5$, which of the following statements is correct?\n",
      "\n",
      "- [a] When $C = 0.0001$, $E_{in}$ is higher at $Q = 5$\n",
      "- **[b] When $C = 0.001$, the number of support vectors is lower at $Q = 5$**(76 vs 25)\n",
      "- [c] When $C = 0.01$, $E_{in}$ is higher at $Q = 5$\n",
      "- [d] When $C = 1$, $E_{out}$ is lower at $Q = 5$\n",
      "- [e] None of the above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C, Q = 0.0001, 5\n",
      "\n",
      "print('C\\te_in\\te_out\\tnsv')\n",
      "while C <= 1:\n",
      "    classifier = SVC(kernel='poly', C=C, degree=Q, shrinking=False, gamma=1, coef0=1)\n",
      "    classifier.fit(subset_X, subset_y)\n",
      "    print('%.4f\\t%.4f\\t%.4f\\t%d' \n",
      "          % (C, 1 - classifier.score(subset_X, subset_y), \n",
      "             1 - classifier.score(subset_tX, subset_ty), np.sum(classifier.n_support_)))\n",
      "    C *= 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C\te_in\te_out\tnsv\n",
        "0.0001\t0.0045\t0.0189\t26\n",
        "0.0010\t0.0045\t0.0212\t25\n",
        "0.0100\t0.0038\t0.0212\t23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.1000\t0.0032\t0.0189\t25"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1.0000\t0.0032\t0.0212\t21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cross Validation\n",
      "\n",
      "In the next two problems, we will experiment with 10-fold cross validation for the polynomial kernel. Because $E_{cv}$ is a random variable that depends on the random partition of the data, we will try 100 runs with different partitions, and base our answer on the number of runs that lead to a particular choice."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 7\n",
      "Consider the 1 versus 5 classifier with $Q = 2$. We use $E_{cv}$ to select \n",
      "$C \\in \\{0.0001, 0.001, 0.01, 0.1, 1\\}$. If there is a tie in $E_{cv}$ , select the smaller $C$. Within the 100 random runs, which of the following statements is correct?\n",
      "\n",
      "- [a] $C = 0.0001$ is selected most often\n",
      "- **[b] $C = 0.001$ is selected most often**\n",
      "- [c] $C = 0.01$ is selected most often\n",
      "- [d] $C = 0.1$ is selected most often\n",
      "- [e] $C = 1$ is selected most often"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "victories = np.zeros(5)  # counter for (0.0001, 0.001, 0.01, 0.1, 1)\n",
      "e_cv_sum = np.zeros(5)\n",
      "\n",
      "for n in range(100):\n",
      "    C, Q = 0.0001, 2\n",
      "    e_cv_each = np.ones(5)\n",
      "    # split the data\n",
      "    kf = KFold(subset_X.shape[0], n_folds=10, shuffle=True)\n",
      "    # compute e_cv for each C\n",
      "    while C <= 1:\n",
      "        i = int(np.log10(C / 0.0001))\n",
      "        classifier = SVC(kernel='poly', C=C, degree=Q, shrinking=False, gamma=1, coef0=1)\n",
      "        # cross validation\n",
      "        for train, validate in kf:\n",
      "            X_t, X_v, y_t, y_v = (subset_X[train], subset_X[validate], \n",
      "                                  subset_y[train], subset_y[validate])\n",
      "            classifier.fit(X_t, y_t)\n",
      "            e = 1 - classifier.score(X_v, y_v)\n",
      "            e_cv_each[i] = e\n",
      "            e_cv_sum[i] += e\n",
      "        C *= 10\n",
      "    victories[np.argmin(e_cv_each)] += 1\n",
      "    \n",
      "print('C:\\t%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f' % (0.0001, 0.001, 0.01, 0.1, 1))\n",
      "print('v:\\t%.0f\\t%.0f\\t%.0f\\t%.0f\\t%.0f' % tuple(victories))\n",
      "print('e_cv:\\t%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f' % tuple(np.array(e_cv_sum)/(10*100)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C:\t0.0001\t0.0010\t0.0100\t0.1000\t1.0000\n",
        "v:\t38\t49\t7\t1\t5\n",
        "e_cv:\t0.0097\t0.0047\t0.0047\t0.0048\t0.0048\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 8\n",
      "Again, consider the 1 versus 5 classifier with $Q = 2$. For the winning selection in the previous problem, the average value of $E_{cv}$ over the 100 runs is closest to\n",
      "\n",
      "- [a] 0.001\n",
      "- [b] 0.003\n",
      "- **[c] 0.005**\n",
      "- [d] 0.007\n",
      "- [e] 0.009"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## RBF Kernel\n",
      "Consider the radial basis function (RBF) kernel $K(x_n, x_m) = exp(\u2212|x_n \u2212 x_m|^2)$. Focus on the 1 versus 5 classifier."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 9\n",
      "Which of the following values of $C$ results in the lowest $E_{in}$ ?\n",
      "\n",
      "- [a] C = 0.01\n",
      "- [b] C = 1\n",
      "- [c] C = 100\n",
      "- [d] C = 10^4\n",
      "- **[e] C = 10^6**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('C\\t\\te_in\\te_out')\n",
      "for C in (0.01, 1, 100, 10**4, 10**6):\n",
      "    classifier = SVC(kernel='rbf', C=C, shrinking=False, gamma=1, coef0=1)\n",
      "    classifier.fit(subset_X, subset_y)\n",
      "    print('%10.2f\\t%.4f\\t%.4f' \n",
      "          % (C, 1 - classifier.score(subset_X, subset_y), \n",
      "             1 - classifier.score(subset_tX, subset_ty)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C\t\te_in\te_out\n",
        "      0.01\t0.0038\t0.0236"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "      1.00\t0.0045\t0.0212\n",
        "    100.00\t0.0032\t0.0189\n",
        "  10000.00\t0.0026\t0.0236"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1000000.00\t0.0006\t0.0236"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 10\n",
      "\n",
      "Which of the following values of $C$ results in the lowest $E_{out}$ ?\n",
      "\n",
      "- [a] C = 0.01\n",
      "- [b] C = 1\n",
      "- **[c] C = 100**\n",
      "- [d] C = 10 4\n",
      "- [e] C = 10 6"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}